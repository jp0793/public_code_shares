class MountAwsS3BucketToAzureDatabricks:

    """
    This method mounts a given AWS S3 bucket into Azure Databricks. Once the data is mmounted, tables can be created on the mounted data and queried as if the data was actually in Databricks File System (DBFS)

    :param aws_s3_bucket: The AWS S3 bucket name
    :param mounting_name: The desired name of the mounting. Its usually good practice to have this in caps e.g. "MY_MOUNT_NAME"
    :param aws_access_key_id: The AWS access key to be used for authentication
    :param aws_secret_key: The AWS secret key (password) accompanying the AWS access key
    :param catalog_name: The name of the catalog of which the tables are to be contained
    :param schema: The name of the schema to contain the table containing the mounted data from a given directory
    :param file_type: The type of files contained in the AWS S3 bucket e.g CSV or Parquet or Excel
    :param directories: A list of directories to be created. These directories are the "folders" contained in the AWS S3 bucket are desired to be created as tables in Azure Databricks workspace
    """

    def __init__(self, aws_access_key_id: str, aws_secret_key: str, aws_s3_bucket: str, mounting_name: str, catalog_name: str, schema: str, file_type: str, directories: list = None):
        
        self.aws_access_key_id = aws_access_key_id
        self.aws_secret_key = aws_secret_key
        self.aws_s3_bucket = aws_s3_bucket
        self.mounting_name = mounting_name
        self.catalog_name = catalog_name
        self.schema = schema
        self.directories = directories
        self.file_type = file_type

    def __encode_secret_key(self):

        """
        This method encodes a given secret key to ensure that the key is safe while in "transit" between the Databricks workspace and AWS S3
        :return: encoded_secret_key
        """

        # Encode Secret Key
        encoded_secret_key = self.aws_secret_access_key.replace("/","%2F")

        return encoded_secret_key

    def mount_bucket(self):

        """
        This method mounts a given AWS S3 bucket to a Databricks workspace in DBFS
        :return: None
        """

        # Mounting of AWS S3 bucket
        dbutils.fs.mount("s3a://%s:%s@%s" % (self.aws_access_key_id, self.__encode_secret_key, self.aws_s3_bucket), "/mnt/%s" % self.mounting_name)

        return None
    
    def view_mounted_directory_content(self):

        """
        This method lists the directories in the mounted AWS S3 bucket
        :return: List of directories in mounted AWS S3 bucket
        """

        # Test the mounting by listing main folders in the AWS S3 bucket
        return dbutils.fs.ls(f"/mnt/{self.mounting_name}/")
    
    def create_tables(self):

        """
        This method creates tables in a given catalog and schema. Each directory will be given as a folder.
        :return: None
        """
        
        # Loop through the list of directories and for each create a corresponding table in a database in a Azure databricks workspace catalog
        for directory in self.directories:

            # Create file path
            file_path = f"/mnt/{self.mounting_name}/{self.directory}/"
            
            # Read the file and infer the schema based on file type
            if self.file_type.lower() == 'parquet':
                df = spark.read.parquet(file_path)
            elif self.file_type.lower() == 'csv':
                df = spark.read.csv(file_path, header=True, inferSchema=True)
            elif self.file_type.lower() == 'excel':
                # Reading Excel files requires the spark-excel package
                df = spark.read.format("com.crealytics.spark.excel").option("header", "true").option("inferSchema", "true").load(file_path)
            else:
                raise ValueError("Unsupported file type. Supported types are: 'parquet', 'csv', 'excel'")

            # Get the schema in DDL format
            schema_ddl = df.schema.toDDL()

            # Create the table in the specified database with the inferred schema
            create_table_query = f"""
            CREATE TABLE {self.catalog_name}.{self.schema}.{self.directory} ({schema_ddl}) 
            USING {self.file_type.upper()} 
            LOCATION '{file_path}'
            """

            # Execute the query to create the table
            spark.sql(create_table_query)

            print(f"Table '{self.catalog_name}.{self.schema}.{self.directory}' created successfully with schema from {file_path}")

            return None
